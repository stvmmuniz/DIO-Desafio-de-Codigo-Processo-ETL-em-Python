# üìä Desafio de C√≥digo ‚Äì Pipeline ETL em Python

Este projeto tem como objetivo demonstrar, de forma pr√°tica, a aplica√ß√£o dos conceitos e boas pr√°ticas de **ETL (Extract, Transform, Load)** utilizando Python. O pipeline foi desenvolvido a partir de dados p√∫blicos do **Portal da Transpar√™ncia**, preparando o dataset final para an√°lise no **Power BI**.

A solu√ß√£o foi estruturada de maneira modular e organizada, facilitando a leitura, manuten√ß√£o e reaproveitamento do c√≥digo, al√©m de evidenciar a capacidade t√©cnica aplicada na resolu√ß√£o do desafio.

<br>

# ‚öôÔ∏è Etapa 0 ‚Äì Configura√ß√£o do Ambiente

Nesta etapa inicial, o ambiente √© preparado para garantir organiza√ß√£o e reprodutibilidade do processo:

Importa√ß√£o de bibliotecas padr√£o do Python (`os`, `zipfile`, `glob`, `unicodedata`) e bibliotecas externas amplamente utilizadas em projetos de dados (`requests` e `pandas`);

Defini√ß√£o de diret√≥rios separados para **dados brutos** (`data_raw`) e **dados processados** (`data_processed`), seguindo boas pr√°ticas de organiza√ß√£o;

Cria√ß√£o autom√°tica dessas pastas, evitando erros em execu√ß√µes futuras.

Essa estrutura facilita tanto o entendimento do fluxo quanto a escalabilidade do projeto.

<br>

# üì• Etapa 1 ‚Äì Extra√ß√£o dos Dados (Extract)

A etapa de extra√ß√£o √© respons√°vel por obter os dados diretamente da fonte oficial:

* **Download automatizado** do arquivo ZIP a partir do Portal da Transpar√™ncia, utilizando requisi√ß√µes HTTP com tratamento de erros;

* **Armazenamento do arquivo bruto localmente**, garantindo rastreabilidade dos dados originais;

* **Extra√ß√£o controlada do conte√∫do do ZIP**, evitando duplicidades;

* **Identifica√ß√£o autom√°tica do arquivo CSV** para processamento.

Essa abordagem garante **confiabilidade** na origem dos dados e reduz depend√™ncias manuais.

<br>

# üîÑ Etapa 2 ‚Äì Transforma√ß√£o dos Dados (Transform)

Esta √© a etapa central do processo, onde os dados brutos s√£o tratados e padronizados para uso anal√≠tico.

## 2.1 Padroniza√ß√£o dos nomes das colunas

Os nomes das colunas s√£o normalizados para remover acentos, espa√ßos e inconsist√™ncias de formata√ß√£o, facilitando o uso em ferramentas anal√≠ticas e evitando erros futuros.

## 2.2 Convers√£o de valores monet√°rios

A coluna de valores financeiros √© convertida corretamente para o tipo num√©rico, respeitando o padr√£o brasileiro de separadores decimais, garantindo precis√£o em c√°lculos e an√°lises.

## 2.3 Tratamento de datas

A coluna de data √© convertida para o formato de data do Python, com tratamento de valores inv√°lidos, assegurando consist√™ncia temporal no dataset.

## 2.4 Convers√£o de c√≥digos num√©ricos

Colunas que representam c√≥digos administrativos s√£o convertidas para o tipo inteiro, preservando valores nulos quando necess√°rio e garantindo melhor desempenho em an√°lises e relacionamentos.

## 2.5 Padroniza√ß√£o de colunas de texto

As colunas textuais passam por limpeza b√°sica, como remo√ß√£o de espa√ßos extras, garantindo maior qualidade dos dados.

### üîç Destaque importante:
A coluna **CodigoFavorecido** foi mantida e tratada como **texto**, pois na base original ela pode conter **valores alfanum√©ricos**. Essa decis√£o evita perda de informa√ß√£o e demonstra aten√ß√£o √† natureza real dos dados, uma pr√°tica essencial em projetos de dados profissionais.

<br>

# üìà Etapa 3 ‚Äì Prepara√ß√£o Final para An√°lise (Load)

Na etapa final, os dados s√£o preparados para consumo no Power BI:

Garantia dos tipos finais das colunas, assegurando compatibilidade com ferramentas de BI;

Sele√ß√£o e ordena√ß√£o das colunas mais relevantes para an√°lise;

Exporta√ß√£o do dataset final em formato CSV, com encoding adequado e separador compat√≠vel com o Power BI.

O arquivo final √© salvo na pasta de dados processados, pronto para visualiza√ß√µes e an√°lises.

<br>

# ‚úÖ Boas Pr√°ticas Aplicadas

Separa√ß√£o clara entre dados brutos e processados;

C√≥digo organizado por etapas do ETL;

Tratamento expl√≠cito de erros e valida√ß√µes;

Padroniza√ß√£o de nomes e tipos de dados;

Decis√µes t√©cnicas baseadas na natureza real dos dados;

Foco em reprodutibilidade, clareza e qualidade dos dados.

<br>

# üß© C√≥digo do Pipeline ETL

A seguir est√£o os arquivos com o c√≥digo completo utilizado neste projeto, organizado por etapas e com coment√°rios que facilitam o entendimento do fluxo de Extra√ß√£o, Transforma√ß√£o e Prepara√ß√£o dos Dados:

<br>

# üìù Arquivos

- [X] Projeto Completo (formato Colab .ipynb): [Clique aqui para visualizar o arquivo.](stvmmuniz_Desafio_de_Codigo_Processo_ETL_em_Python.ipynb)

- [X] Python (formato .py): [Clique aqui para visualizar o arquivo.](stvmmuniz_Desafio_de_Codigo_Processo_ETL_em_Python.py)
<br><br>

# üéØ Conclus√£o

Este projeto demonstra a aplica√ß√£o pr√°tica de um pipeline ETL completo, desde a extra√ß√£o de dados p√∫blicos at√© a entrega de um dataset pronto para an√°lise. A solu√ß√£o equilibra **qualidade t√©cnica, clareza de implementa√ß√£o e boas pr√°ticas de engenharia de dados**, sendo adequada tanto para avalia√ß√£o t√©cnica quanto para apresenta√ß√£o em um portf√≥lio profissional.